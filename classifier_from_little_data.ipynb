{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[4096]\n\t [[Node: fc1/bias/Assign = Assign[T=DT_FLOAT, _class=[\"loc:@fc1/bias\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](fc1/bias, fc1/Const)]]\n\nCaused by op u'fc1/bias/Assign', defined at:\n  File \"/usr/lib/python2.7/runpy.py\", line 162, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/home/yujia/env/tensorflow/lib/python2.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/ioloop.py\", line 160, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 433, in _handle_events\n    self._handle_recv()\n  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 465, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 407, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-1-075dc424ddd1>\", line 9, in <module>\n    model = VGG16(weights='imagenet')\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/keras/applications/vgg16.py\", line 142, in VGG16\n    x = Dense(4096, activation='relu', name='fc1')(x)\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/keras/engine/topology.py\", line 575, in __call__\n    self.build(input_shapes[0])\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/keras/layers/core.py\", line 834, in build\n    constraint=self.bias_constraint)\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/keras/legacy/interfaces.py\", line 87, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/keras/engine/topology.py\", line 399, in add_weight\n    constraint=constraint)\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py\", line 316, in variable\n    v = tf.Variable(value, dtype=_convert_string_dtype(dtype), name=name)\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/variables.py\", line 213, in __init__\n    constraint=constraint)\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/variables.py\", line 346, in _init_from_args\n    validate_shape=validate_shape).op\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/state_ops.py\", line 274, in assign\n    validate_shape=validate_shape)\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_state_ops.py\", line 56, in assign\n    use_locking=use_locking, name=name)\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 3046, in create_op\n    op_def=op_def)\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1604, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[4096]\n\t [[Node: fc1/bias/Assign = Assign[T=DT_FLOAT, _class=[\"loc:@fc1/bias\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](fc1/bias, fc1/Const)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-075dc424ddd1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVGG16\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'imagenet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mimg_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'demo.jpg'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/keras/applications/vgg16.pyc\u001b[0m in \u001b[0;36mVGG16\u001b[0;34m(include_top, weights, input_tensor, input_shape, pooling, classes)\u001b[0m\n\u001b[1;32m    168\u001b[0m                                     \u001b[0mWEIGHTS_PATH_NO_TOP\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m                                     cache_subdir='models')\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'theano'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0mlayer_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_all_kernels_in_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/keras/engine/topology.pyc\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name)\u001b[0m\n\u001b[1;32m   2617\u001b[0m             \u001b[0mload_weights_from_hdf5_group_by_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2618\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2619\u001b[0;31m             \u001b[0mload_weights_from_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2620\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2621\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'close'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/keras/engine/topology.pyc\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[0;34m(f, layers)\u001b[0m\n\u001b[1;32m   3093\u001b[0m                              ' elements.')\n\u001b[1;32m   3094\u001b[0m         \u001b[0mweight_value_tuples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbolic_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3095\u001b[0;31m     \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_set_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_value_tuples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3096\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3097\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36mbatch_set_value\u001b[0;34m(tuples)\u001b[0m\n\u001b[1;32m   2191\u001b[0m             \u001b[0massign_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massign_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2192\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0massign_placeholder\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2193\u001b[0;31m         \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massign_ops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_MANUAL_VAR_INIT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m             \u001b[0m_initialize_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m_initialize_variables\u001b[0;34m()\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muninitialized_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muninitialized_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1116\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1117\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1118\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1119\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1120\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1313\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1315\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1316\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[4096]\n\t [[Node: fc1/bias/Assign = Assign[T=DT_FLOAT, _class=[\"loc:@fc1/bias\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](fc1/bias, fc1/Const)]]\n\nCaused by op u'fc1/bias/Assign', defined at:\n  File \"/usr/lib/python2.7/runpy.py\", line 162, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/home/yujia/env/tensorflow/lib/python2.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/ioloop.py\", line 160, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 433, in _handle_events\n    self._handle_recv()\n  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 465, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 407, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-1-075dc424ddd1>\", line 9, in <module>\n    model = VGG16(weights='imagenet')\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/keras/applications/vgg16.py\", line 142, in VGG16\n    x = Dense(4096, activation='relu', name='fc1')(x)\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/keras/engine/topology.py\", line 575, in __call__\n    self.build(input_shapes[0])\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/keras/layers/core.py\", line 834, in build\n    constraint=self.bias_constraint)\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/keras/legacy/interfaces.py\", line 87, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/keras/engine/topology.py\", line 399, in add_weight\n    constraint=constraint)\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py\", line 316, in variable\n    v = tf.Variable(value, dtype=_convert_string_dtype(dtype), name=name)\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/variables.py\", line 213, in __init__\n    constraint=constraint)\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/variables.py\", line 346, in _init_from_args\n    validate_shape=validate_shape).op\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/state_ops.py\", line 274, in assign\n    validate_shape=validate_shape)\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_state_ops.py\", line 56, in assign\n    use_locking=use_locking, name=name)\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 3046, in create_op\n    op_def=op_def)\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1604, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[4096]\n\t [[Node: fc1/bias/Assign = Assign[T=DT_FLOAT, _class=[\"loc:@fc1/bias\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](fc1/bias, fc1/Const)]]\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input, decode_predictions\n",
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "model = VGG16(weights='imagenet')\n",
    "\n",
    "img_path = 'demo.jpg'\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n",
    "\n",
    "features = model.predict(x)\n",
    "preds = decode_predictions(features, top=1)[0][0]\n",
    "print(preds)\n",
    "\n",
    "\n",
    "img = cv2.imread('demo.jpg',cv2.IMREAD_COLOR)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) \n",
    "cv2.putText(img, \"Label: {}, {:.2f}%\".format(preds[1], preds[2] * 100), \n",
    "            (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0,255,0), 2, cv2.LINE_AA)\n",
    "plt.figure(dpi=150)\n",
    "plt.imshow(img, cmap = 'gray', interpolation = 'bicubic')\n",
    "plt.xticks([]), plt.yticks([])  # to hide tick values on X and Y axis\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 800 images belonging to 2 classes.\n",
      "Epoch 1/50\n",
      "125/125 [==============================] - 145s - loss: 0.7153 - acc: 0.5315 - val_loss: 0.6875 - val_acc: 0.6150\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - 21s - loss: 0.6827 - acc: 0.5915 - val_loss: 0.7204 - val_acc: 0.5337\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - 21s - loss: 0.6462 - acc: 0.6385 - val_loss: 0.6042 - val_acc: 0.6700\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - 21s - loss: 0.6110 - acc: 0.6650 - val_loss: 0.5713 - val_acc: 0.6863\n",
      "Epoch 5/50\n",
      "125/125 [==============================] - 21s - loss: 0.5759 - acc: 0.7105 - val_loss: 0.5678 - val_acc: 0.7150\n",
      "Epoch 6/50\n",
      "125/125 [==============================] - 22s - loss: 0.5659 - acc: 0.7115 - val_loss: 0.5516 - val_acc: 0.7087\n",
      "Epoch 7/50\n",
      "125/125 [==============================] - 21s - loss: 0.5476 - acc: 0.7110 - val_loss: 0.5730 - val_acc: 0.7238\n",
      "Epoch 8/50\n",
      "125/125 [==============================] - 21s - loss: 0.5410 - acc: 0.7295 - val_loss: 0.5744 - val_acc: 0.6875\n",
      "Epoch 9/50\n",
      "125/125 [==============================] - 22s - loss: 0.5243 - acc: 0.7425 - val_loss: 0.5466 - val_acc: 0.7362\n",
      "Epoch 10/50\n",
      "125/125 [==============================] - 22s - loss: 0.5118 - acc: 0.7545 - val_loss: 0.5297 - val_acc: 0.7388\n",
      "Epoch 11/50\n",
      "125/125 [==============================] - 22s - loss: 0.5154 - acc: 0.7600 - val_loss: 0.5025 - val_acc: 0.7675\n",
      "Epoch 12/50\n",
      "125/125 [==============================] - 22s - loss: 0.4955 - acc: 0.7575 - val_loss: 0.5184 - val_acc: 0.7388\n",
      "Epoch 13/50\n",
      "125/125 [==============================] - 22s - loss: 0.4812 - acc: 0.7805 - val_loss: 0.5269 - val_acc: 0.7500\n",
      "Epoch 14/50\n",
      "125/125 [==============================] - 22s - loss: 0.4801 - acc: 0.7855 - val_loss: 0.5603 - val_acc: 0.7137\n",
      "Epoch 15/50\n",
      "125/125 [==============================] - 21s - loss: 0.4719 - acc: 0.7840 - val_loss: 0.5032 - val_acc: 0.7738\n",
      "Epoch 16/50\n",
      "125/125 [==============================] - 21s - loss: 0.4607 - acc: 0.7890 - val_loss: 0.5128 - val_acc: 0.7425\n",
      "Epoch 17/50\n",
      "125/125 [==============================] - 21s - loss: 0.4443 - acc: 0.7915 - val_loss: 0.5113 - val_acc: 0.7700\n",
      "Epoch 18/50\n",
      "125/125 [==============================] - 21s - loss: 0.4610 - acc: 0.7880 - val_loss: 0.5311 - val_acc: 0.7438\n",
      "Epoch 19/50\n",
      "125/125 [==============================] - 21s - loss: 0.4623 - acc: 0.7995 - val_loss: 0.5111 - val_acc: 0.7975\n",
      "Epoch 20/50\n",
      "125/125 [==============================] - 21s - loss: 0.4496 - acc: 0.7905 - val_loss: 0.6345 - val_acc: 0.7500\n",
      "Epoch 21/50\n",
      "125/125 [==============================] - 21s - loss: 0.4422 - acc: 0.8090 - val_loss: 0.4577 - val_acc: 0.8063\n",
      "Epoch 22/50\n",
      "125/125 [==============================] - 21s - loss: 0.4458 - acc: 0.8050 - val_loss: 0.4682 - val_acc: 0.7612\n",
      "Epoch 23/50\n",
      "125/125 [==============================] - 21s - loss: 0.4314 - acc: 0.8050 - val_loss: 0.4538 - val_acc: 0.8050\n",
      "Epoch 24/50\n",
      "125/125 [==============================] - 21s - loss: 0.4344 - acc: 0.8110 - val_loss: 0.5177 - val_acc: 0.7700\n",
      "Epoch 25/50\n",
      "125/125 [==============================] - 23s - loss: 0.4546 - acc: 0.8125 - val_loss: 0.4895 - val_acc: 0.7837\n",
      "Epoch 26/50\n",
      "125/125 [==============================] - 25s - loss: 0.4262 - acc: 0.8125 - val_loss: 0.4522 - val_acc: 0.8013\n",
      "Epoch 27/50\n",
      "125/125 [==============================] - 23s - loss: 0.4534 - acc: 0.8220 - val_loss: 0.5097 - val_acc: 0.7837\n",
      "Epoch 28/50\n",
      "125/125 [==============================] - 22s - loss: 0.4376 - acc: 0.8045 - val_loss: 0.4968 - val_acc: 0.8063\n",
      "Epoch 29/50\n",
      "125/125 [==============================] - 22s - loss: 0.4223 - acc: 0.8225 - val_loss: 0.5450 - val_acc: 0.7462\n",
      "Epoch 30/50\n",
      "125/125 [==============================] - 22s - loss: 0.4258 - acc: 0.8240 - val_loss: 0.5014 - val_acc: 0.7500\n",
      "Epoch 31/50\n",
      "125/125 [==============================] - 22s - loss: 0.4217 - acc: 0.8240 - val_loss: 0.5030 - val_acc: 0.7638\n",
      "Epoch 32/50\n",
      "125/125 [==============================] - 22s - loss: 0.4267 - acc: 0.8300 - val_loss: 0.5225 - val_acc: 0.7450\n",
      "Epoch 33/50\n",
      "125/125 [==============================] - 22s - loss: 0.4017 - acc: 0.8415 - val_loss: 0.6443 - val_acc: 0.8100\n",
      "Epoch 34/50\n",
      "125/125 [==============================] - 22s - loss: 0.4332 - acc: 0.8195 - val_loss: 0.4794 - val_acc: 0.7738\n",
      "Epoch 35/50\n",
      "125/125 [==============================] - 23s - loss: 0.3947 - acc: 0.8270 - val_loss: 0.4641 - val_acc: 0.7975\n",
      "Epoch 36/50\n",
      "125/125 [==============================] - 22s - loss: 0.4160 - acc: 0.8295 - val_loss: 0.7011 - val_acc: 0.7475\n",
      "Epoch 37/50\n",
      "125/125 [==============================] - 22s - loss: 0.4188 - acc: 0.8205 - val_loss: 0.4860 - val_acc: 0.8125\n",
      "Epoch 38/50\n",
      "125/125 [==============================] - 22s - loss: 0.4127 - acc: 0.8310 - val_loss: 0.5352 - val_acc: 0.7700\n",
      "Epoch 39/50\n",
      "125/125 [==============================] - 22s - loss: 0.4277 - acc: 0.8140 - val_loss: 0.5171 - val_acc: 0.7800\n",
      "Epoch 40/50\n",
      "125/125 [==============================] - 22s - loss: 0.4255 - acc: 0.8185 - val_loss: 0.5087 - val_acc: 0.8025\n",
      "Epoch 41/50\n",
      "125/125 [==============================] - 22s - loss: 0.3959 - acc: 0.8410 - val_loss: 0.5249 - val_acc: 0.7925\n",
      "Epoch 42/50\n",
      "125/125 [==============================] - 22s - loss: 0.4233 - acc: 0.8325 - val_loss: 0.4987 - val_acc: 0.7837\n",
      "Epoch 43/50\n",
      "125/125 [==============================] - 22s - loss: 0.4238 - acc: 0.8305 - val_loss: 0.5438 - val_acc: 0.7388\n",
      "Epoch 44/50\n",
      "125/125 [==============================] - 22s - loss: 0.4264 - acc: 0.8200 - val_loss: 0.6191 - val_acc: 0.7788\n",
      "Epoch 45/50\n",
      "125/125 [==============================] - 22s - loss: 0.4185 - acc: 0.8265 - val_loss: 0.4982 - val_acc: 0.8000\n",
      "Epoch 46/50\n",
      "125/125 [==============================] - 21s - loss: 0.4261 - acc: 0.8275 - val_loss: 0.7139 - val_acc: 0.6987\n",
      "Epoch 47/50\n",
      "125/125 [==============================] - 21s - loss: 0.4221 - acc: 0.8320 - val_loss: 0.5792 - val_acc: 0.7913\n",
      "Epoch 48/50\n",
      "125/125 [==============================] - 21s - loss: 0.4032 - acc: 0.8315 - val_loss: 0.7660 - val_acc: 0.7250\n",
      "Epoch 49/50\n",
      "125/125 [==============================] - 21s - loss: 0.4067 - acc: 0.8370 - val_loss: 0.5704 - val_acc: 0.7775\n",
      "Epoch 50/50\n",
      "125/125 [==============================] - 21s - loss: 0.4099 - acc: 0.8265 - val_loss: 0.5152 - val_acc: 0.7925\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "# dimensions of our images.\n",
    "img_width, img_height = 150, 150\n",
    "\n",
    "train_data_dir = 'data/train'\n",
    "validation_data_dir = 'data/validation'\n",
    "nb_train_samples = 2000\n",
    "nb_validation_samples = 800\n",
    "epochs = 50\n",
    "batch_size = 16\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    input_shape = (3, img_width, img_height)\n",
    "else:\n",
    "    input_shape = (img_width, img_height, 3)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "# this is the augmentation configuration we will use for testing:\n",
    "# only rescaling\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=nb_train_samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=nb_validation_samples // batch_size)\n",
    "\n",
    "model.save_weights('first_try.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.bottle_neck train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58859520/58889256 [============================>.] - ETA: 0sFound 2000 images belonging to 2 classes.\n",
      "Found 800 images belonging to 2 classes.\n",
      "Train on 2000 samples, validate on 800 samples\n",
      "Epoch 1/50\n",
      "2000/2000 [==============================] - 3s - loss: 0.6957 - acc: 0.7600 - val_loss: 0.3140 - val_acc: 0.8550\n",
      "Epoch 2/50\n",
      "2000/2000 [==============================] - 3s - loss: 0.3581 - acc: 0.8620 - val_loss: 0.2382 - val_acc: 0.9050\n",
      "Epoch 3/50\n",
      "2000/2000 [==============================] - 4s - loss: 0.3095 - acc: 0.8880 - val_loss: 0.2810 - val_acc: 0.8975\n",
      "Epoch 4/50\n",
      "2000/2000 [==============================] - 3s - loss: 0.2647 - acc: 0.8930 - val_loss: 0.2971 - val_acc: 0.8950\n",
      "Epoch 5/50\n",
      "2000/2000 [==============================] - 3s - loss: 0.2178 - acc: 0.9065 - val_loss: 0.3916 - val_acc: 0.8712\n",
      "Epoch 6/50\n",
      "2000/2000 [==============================] - 3s - loss: 0.2062 - acc: 0.9250 - val_loss: 0.2856 - val_acc: 0.8950\n",
      "Epoch 7/50\n",
      "2000/2000 [==============================] - 3s - loss: 0.1579 - acc: 0.9415 - val_loss: 0.4591 - val_acc: 0.8738\n",
      "Epoch 8/50\n",
      "2000/2000 [==============================] - 3s - loss: 0.1767 - acc: 0.9400 - val_loss: 0.3302 - val_acc: 0.9025\n",
      "Epoch 9/50\n",
      "2000/2000 [==============================] - 3s - loss: 0.1328 - acc: 0.9500 - val_loss: 0.6089 - val_acc: 0.8575\n",
      "Epoch 10/50\n",
      "2000/2000 [==============================] - 3s - loss: 0.1348 - acc: 0.9515 - val_loss: 0.6352 - val_acc: 0.8325\n",
      "Epoch 11/50\n",
      "2000/2000 [==============================] - 3s - loss: 0.1093 - acc: 0.9600 - val_loss: 0.3879 - val_acc: 0.8888\n",
      "Epoch 12/50\n",
      "2000/2000 [==============================] - 3s - loss: 0.1067 - acc: 0.9635 - val_loss: 0.4521 - val_acc: 0.8825\n",
      "Epoch 13/50\n",
      "2000/2000 [==============================] - 3s - loss: 0.0791 - acc: 0.9660 - val_loss: 0.5190 - val_acc: 0.8900\n",
      "Epoch 14/50\n",
      "2000/2000 [==============================] - 3s - loss: 0.0930 - acc: 0.9660 - val_loss: 0.5095 - val_acc: 0.8962\n",
      "Epoch 15/50\n",
      "2000/2000 [==============================] - 3s - loss: 0.0674 - acc: 0.9715 - val_loss: 0.4794 - val_acc: 0.9050\n",
      "Epoch 16/50\n",
      "2000/2000 [==============================] - 3s - loss: 0.0629 - acc: 0.9815 - val_loss: 0.5450 - val_acc: 0.9000\n",
      "Epoch 17/50\n",
      "2000/2000 [==============================] - 3s - loss: 0.0736 - acc: 0.9770 - val_loss: 0.5640 - val_acc: 0.9038\n",
      "Epoch 18/50\n",
      "2000/2000 [==============================] - 3s - loss: 0.0640 - acc: 0.9795 - val_loss: 0.6399 - val_acc: 0.8838\n",
      "Epoch 19/50\n",
      "2000/2000 [==============================] - 3s - loss: 0.0518 - acc: 0.9795 - val_loss: 0.5288 - val_acc: 0.90250.\n",
      "Epoch 20/50\n",
      "2000/2000 [==============================] - 3s - loss: 0.0480 - acc: 0.9835 - val_loss: 0.7855 - val_acc: 0.8725\n",
      "Epoch 21/50\n",
      "2000/2000 [==============================] - 3s - loss: 0.0495 - acc: 0.9865 - val_loss: 1.0541 - val_acc: 0.8550\n",
      "Epoch 22/50\n",
      "2000/2000 [==============================] - 3s - loss: 0.0444 - acc: 0.9870 - val_loss: 0.6947 - val_acc: 0.9012\n",
      "Epoch 23/50\n",
      "2000/2000 [==============================] - 3s - loss: 0.0380 - acc: 0.9875 - val_loss: 0.9240 - val_acc: 0.8700\n",
      "Epoch 24/50\n",
      "2000/2000 [==============================] - 3s - loss: 0.0376 - acc: 0.9865 - val_loss: 0.9546 - val_acc: 0.8675\n",
      "Epoch 25/50\n",
      "2000/2000 [==============================] - 3s - loss: 0.0335 - acc: 0.9895 - val_loss: 0.7017 - val_acc: 0.8875\n",
      "Epoch 26/50\n",
      "2000/2000 [==============================] - 3s - loss: 0.0295 - acc: 0.9880 - val_loss: 0.8264 - val_acc: 0.8838\n",
      "Epoch 27/50\n",
      "2000/2000 [==============================] - 3s - loss: 0.0251 - acc: 0.9915 - val_loss: 0.7304 - val_acc: 0.9062\n",
      "Epoch 28/50\n",
      "2000/2000 [==============================] - 3s - loss: 0.0451 - acc: 0.9880 - val_loss: 0.7321 - val_acc: 0.9062\n",
      "Epoch 29/50\n",
      "2000/2000 [==============================] - 3s - loss: 0.0098 - acc: 0.9960 - val_loss: 0.7564 - val_acc: 0.9050\n",
      "Epoch 30/50\n",
      "2000/2000 [==============================] - 3s - loss: 0.0329 - acc: 0.9895 - val_loss: 0.7075 - val_acc: 0.9150\n",
      "Epoch 31/50\n",
      "2000/2000 [==============================] - 3s - loss: 0.0372 - acc: 0.9890 - val_loss: 0.7214 - val_acc: 0.9137\n",
      "Epoch 32/50\n",
      "2000/2000 [==============================] - 3s - loss: 0.0197 - acc: 0.9940 - val_loss: 0.9090 - val_acc: 0.8925\n",
      "Epoch 33/50\n",
      "2000/2000 [==============================] - 3s - loss: 0.0225 - acc: 0.9920 - val_loss: 0.8804 - val_acc: 0.8988\n",
      "Epoch 34/50\n",
      "2000/2000 [==============================] - 3s - loss: 0.0367 - acc: 0.9910 - val_loss: 0.7686 - val_acc: 0.9100\n",
      "Epoch 35/50\n",
      "2000/2000 [==============================] - 3s - loss: 0.0158 - acc: 0.9945 - val_loss: 0.8572 - val_acc: 0.9075\n",
      "Epoch 36/50\n",
      "2000/2000 [==============================] - 3s - loss: 0.0061 - acc: 0.9970 - val_loss: 0.8514 - val_acc: 0.9075\n",
      "Epoch 37/50\n",
      "2000/2000 [==============================] - 3s - loss: 0.0156 - acc: 0.9950 - val_loss: 0.8805 - val_acc: 0.9012\n",
      "Epoch 38/50\n",
      "2000/2000 [==============================] - 3s - loss: 0.0130 - acc: 0.9950 - val_loss: 0.8299 - val_acc: 0.9087\n",
      "Epoch 39/50\n",
      "2000/2000 [==============================] - 3s - loss: 0.0115 - acc: 0.9970 - val_loss: 0.8855 - val_acc: 0.9038\n",
      "Epoch 40/50\n",
      "2000/2000 [==============================] - 3s - loss: 0.0241 - acc: 0.9920 - val_loss: 0.9006 - val_acc: 0.9012\n",
      "Epoch 41/50\n",
      "2000/2000 [==============================] - 3s - loss: 0.0123 - acc: 0.9955 - val_loss: 0.8664 - val_acc: 0.9075\n",
      "Epoch 42/50\n",
      "2000/2000 [==============================] - 3s - loss: 0.0095 - acc: 0.9975 - val_loss: 0.9275 - val_acc: 0.9062\n",
      "Epoch 43/50\n",
      "2000/2000 [==============================] - 3s - loss: 0.0234 - acc: 0.9950 - val_loss: 1.1723 - val_acc: 0.8762\n",
      "Epoch 44/50\n",
      "2000/2000 [==============================] - 4s - loss: 0.0248 - acc: 0.9935 - val_loss: 0.9209 - val_acc: 0.9025\n",
      "Epoch 45/50\n",
      "2000/2000 [==============================] - 4s - loss: 0.0163 - acc: 0.9945 - val_loss: 0.8950 - val_acc: 0.9050\n",
      "Epoch 46/50\n",
      "2000/2000 [==============================] - 3s - loss: 0.0121 - acc: 0.9960 - val_loss: 0.9334 - val_acc: 0.9100\n",
      "Epoch 47/50\n",
      "2000/2000 [==============================] - 3s - loss: 0.0083 - acc: 0.9980 - val_loss: 1.0309 - val_acc: 0.9038\n",
      "Epoch 48/50\n",
      "2000/2000 [==============================] - 3s - loss: 0.0183 - acc: 0.9960 - val_loss: 1.0623 - val_acc: 0.8888\n",
      "Epoch 49/50\n",
      "2000/2000 [==============================] - 3s - loss: 0.0077 - acc: 0.9980 - val_loss: 0.9220 - val_acc: 0.9025\n",
      "Epoch 50/50\n",
      "2000/2000 [==============================] - 4s - loss: 0.0164 - acc: 0.9955 - val_loss: 1.1006 - val_acc: 0.8912\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras import applications\n",
    "\n",
    "# dimensions of our images.\n",
    "img_width, img_height = 150, 150\n",
    "\n",
    "top_model_weights_path = 'bottleneck_fc_model.h5'\n",
    "train_data_dir = 'data/train'\n",
    "validation_data_dir = 'data/validation'\n",
    "nb_train_samples = 2000\n",
    "nb_validation_samples = 800\n",
    "epochs = 50\n",
    "batch_size = 16\n",
    "\n",
    "\n",
    "def save_bottlebeck_features():\n",
    "    datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "    # build the VGG16 network\n",
    "    model = applications.VGG16(include_top=False, weights='imagenet')\n",
    "\n",
    "    generator = datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,\n",
    "        shuffle=False)\n",
    "    bottleneck_features_train = model.predict_generator(\n",
    "        generator, nb_train_samples // batch_size)\n",
    "    np.save(open('bottleneck_features_train.npy', 'w'),\n",
    "            bottleneck_features_train)\n",
    "\n",
    "    generator = datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,\n",
    "        shuffle=False)\n",
    "    bottleneck_features_validation = model.predict_generator(\n",
    "        generator, nb_validation_samples // batch_size)\n",
    "    np.save(open('bottleneck_features_validation.npy', 'w'),\n",
    "            bottleneck_features_validation)\n",
    "\n",
    "\n",
    "def train_top_model():\n",
    "    train_data = np.load(open('bottleneck_features_train.npy'))\n",
    "    train_labels = np.array(\n",
    "        [0] * (nb_train_samples / 2) + [1] * (nb_train_samples / 2))\n",
    "\n",
    "    validation_data = np.load(open('bottleneck_features_validation.npy'))\n",
    "    validation_labels = np.array(\n",
    "        [0] * (nb_validation_samples / 2) + [1] * (nb_validation_samples / 2))\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=train_data.shape[1:]))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(optimizer='rmsprop',\n",
    "                  loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    model.fit(train_data, train_labels,\n",
    "              epochs=epochs,\n",
    "              batch_size=batch_size,\n",
    "              validation_data=(validation_data, validation_labels))\n",
    "    model.save_weights(top_model_weights_path)\n",
    "\n",
    "\n",
    "save_bottlebeck_features()\n",
    "train_top_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.fine tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n",
      "Found 2000 images belonging to 2 classes.\n",
      "Found 800 images belonging to 2 classes.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 150, 150, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 150, 150, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 150, 150, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 75, 75, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 75, 75, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 75, 75, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 37, 37, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 37, 37, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 18, 18, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 18, 18, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 9, 9, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "sequential_1 (Sequential)    (None, 1)                 2097665   \n",
      "=================================================================\n",
      "Total params: 16,812,353\n",
      "Trainable params: 9,177,089\n",
      "Non-trainable params: 7,635,264\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "117s - loss: 0.4251 - acc: 0.8570 - val_loss: 0.4796 - val_acc: 0.8788\n",
      "Epoch 2/50\n",
      "117s - loss: 0.2659 - acc: 0.9005 - val_loss: 0.3343 - val_acc: 0.8950\n",
      "Epoch 3/50\n",
      "111s - loss: 0.1833 - acc: 0.9280 - val_loss: 0.2949 - val_acc: 0.8962\n",
      "Epoch 4/50\n",
      "113s - loss: 0.1657 - acc: 0.9445 - val_loss: 0.2689 - val_acc: 0.9187\n",
      "Epoch 5/50\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[16,64,150,150]\n\t [[Node: block1_conv2/convolution = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](block1_conv1/Relu, block1_conv2/kernel/read)]]\n\nCaused by op u'block1_conv2/convolution', defined at:\n  File \"/usr/lib/python2.7/runpy.py\", line 162, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/home/yujia/env/tensorflow/lib/python2.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/ioloop.py\", line 160, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 433, in _handle_events\n    self._handle_recv()\n  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 465, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 407, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-1-707755f4b059>\", line 21, in <module>\n    base_model = applications.VGG16(weights='imagenet', include_top=False, input_shape=(150,150,3))\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/keras/applications/vgg16.py\", line 113, in VGG16\n    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/keras/engine/topology.py\", line 602, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/keras/layers/convolutional.py\", line 164, in call\n    dilation_rate=self.dilation_rate)\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py\", line 3164, in conv2d\n    data_format='NHWC')\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/nn_ops.py\", line 672, in convolution\n    op=op)\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/nn_ops.py\", line 338, in with_space_to_batch\n    return op(input, num_spatial_dims, padding)\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/nn_ops.py\", line 664, in op\n    name=name)\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/nn_ops.py\", line 131, in _non_atrous_convolution\n    name=name)\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 619, in conv2d\n    data_format=data_format, name=name)\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 3046, in create_op\n    op_def=op_def)\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1604, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[16,64,150,150]\n\t [[Node: block1_conv2/convolution = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](block1_conv1/Relu, block1_conv2/kernel/read)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-707755f4b059>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_validation_samples\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m     verbose=2)\n\u001b[0m",
      "\u001b[0;32m/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/keras/legacy/interfaces.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2040\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   2041\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2042\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   2043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2044\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1760\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1761\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1762\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1763\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1764\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2271\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2272\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2273\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2274\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1116\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1117\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1118\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1119\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1120\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1313\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1315\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1316\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[16,64,150,150]\n\t [[Node: block1_conv2/convolution = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](block1_conv1/Relu, block1_conv2/kernel/read)]]\n\nCaused by op u'block1_conv2/convolution', defined at:\n  File \"/usr/lib/python2.7/runpy.py\", line 162, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/home/yujia/env/tensorflow/lib/python2.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/ioloop.py\", line 160, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 433, in _handle_events\n    self._handle_recv()\n  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 465, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 407, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-1-707755f4b059>\", line 21, in <module>\n    base_model = applications.VGG16(weights='imagenet', include_top=False, input_shape=(150,150,3))\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/keras/applications/vgg16.py\", line 113, in VGG16\n    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/keras/engine/topology.py\", line 602, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/keras/layers/convolutional.py\", line 164, in call\n    dilation_rate=self.dilation_rate)\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py\", line 3164, in conv2d\n    data_format='NHWC')\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/nn_ops.py\", line 672, in convolution\n    op=op)\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/nn_ops.py\", line 338, in with_space_to_batch\n    return op(input, num_spatial_dims, padding)\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/nn_ops.py\", line 664, in op\n    name=name)\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/nn_ops.py\", line 131, in _non_atrous_convolution\n    name=name)\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 619, in conv2d\n    data_format=data_format, name=name)\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 3046, in create_op\n    op_def=op_def)\n  File \"/home/yujia/env/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1604, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[16,64,150,150]\n\t [[Node: block1_conv2/convolution = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](block1_conv1/Relu, block1_conv2/kernel/read)]]\n"
     ]
    }
   ],
   "source": [
    "from keras import applications\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "\n",
    "# path to the model weights files.\n",
    "top_model_weights_path = 'bottleneck_fc_model.h5'\n",
    "# dimensions of our images.\n",
    "img_width, img_height = 150, 150\n",
    "\n",
    "train_data_dir = 'data/train'\n",
    "validation_data_dir = 'data/validation'\n",
    "nb_train_samples = 2000\n",
    "nb_validation_samples = 800\n",
    "epochs = 50\n",
    "batch_size = 16\n",
    "\n",
    "# build the VGG16 network\n",
    "base_model = applications.VGG16(weights='imagenet', include_top=False, input_shape=(150,150,3))\n",
    "print('Model loaded.')\n",
    "\n",
    "# build a classifier model to put on top of the convolutional model\n",
    "top_model = Sequential()\n",
    "top_model.add(Flatten(input_shape=base_model.output_shape[1:]))\n",
    "top_model.add(Dense(256, activation='relu'))\n",
    "top_model.add(Dropout(0.5))\n",
    "top_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# note that it is necessary to start with a fully-trained\n",
    "# classifier, including the top classifier,\n",
    "# in order to successfully do fine-tuning\n",
    "top_model.load_weights(top_model_weights_path)\n",
    "\n",
    "# add the model on top of the convolutional base\n",
    "# model.add(top_model)\n",
    "model = Model(inputs=base_model.input, outputs=top_model(base_model.output))\n",
    "\n",
    "# set the first 25 layers (up to the last conv block)\n",
    "# to non-trainable (weights will not be updated)\n",
    "for layer in model.layers[:15]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# compile the model with a SGD/momentum optimizer\n",
    "# and a very slow learning rate.\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# prepare data augmentation configuration\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# fine-tune the model\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=nb_train_samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=nb_validation_samples // batch_size,\n",
    "    verbose=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
